{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Dark-Sied/Intent_Classification/blob/master/Intent_classification_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Classification using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a_WypuUXi92e",
    "outputId": "133d026e-4236-4ff6-f21d-739bfb9640db"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, GRU, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset link: https://raw.githubusercontent.com/PolyAI-LDN/task-specific-datasets/master/banking_data/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Intent = \"category\"\n",
    "Sentence = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LE6wywJrN2ih"
   },
   "outputs": [],
   "source": [
    "def load_dataset(filename, Sentence, Intent):\n",
    "  df = pd.read_csv(filename, names = [Sentence, Intent])\n",
    "  intent = df[Intent]\n",
    "  unique_intent = list(set(intent))\n",
    "  sentences = list(df[Sentence])\n",
    "  \n",
    "  return (df, intent, unique_intent, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "tF0FQA7gjOCX",
    "outputId": "c609b42a-05da-49f5-8d11-bd670210f635"
   },
   "outputs": [],
   "source": [
    "df, intent, unique_intent, sentences = load_dataset(\"Dataset.csv\", \"text\", \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text      category\n",
      "0                                               text      category\n",
      "1                     I am still waiting on my card?  card_arrival\n",
      "2  What can I do if my card still hasn't arrived ...  card_arrival\n",
      "3  I have been waiting over a week. Is the card s...  card_arrival\n",
      "4  Can I track my card while it is in the process...  card_arrival\n",
      "5  How do I know if I will get my card, or if it ...  card_arrival\n",
      "6                  When did you send me my new card?  card_arrival\n",
      "7       Do you have info about the card on delivery?  card_arrival\n",
      "8  What do I do if I still have not received my n...  card_arrival\n",
      "9       Does the package with my card have tracking?  card_arrival\n"
     ]
    }
   ],
   "source": [
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='category', ylabel='count'>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import tkinter\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.countplot(x=Intent, data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O8LLUZlokg0S",
    "outputId": "c15c21dc-2ef2-43b7-b4af-e7ee9e014091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'I am still waiting on my card?', \"What can I do if my card still hasn't arrived after 2 weeks?\", 'I have been waiting over a week. Is the card still coming?', 'Can I track my card while it is in the process of delivery?']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "MhrziINPGHbW",
    "outputId": "0861af1b-4b82-4c92-b8f4-b6b57bb3e380"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shiningflash/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/shiningflash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmNLu2YSXePb"
   },
   "outputs": [],
   "source": [
    "#define stemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-7q3iG5PKYI"
   },
   "outputs": [],
   "source": [
    "def cleaning(sentences):\n",
    "  words = []\n",
    "  for s in sentences:\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "    w = word_tokenize(clean)\n",
    "    words.append([i.lower() for i in w])\n",
    "    \n",
    "  return words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "p1j2GJgDG6qj",
    "outputId": "c7232a8e-6833-4a1d-e71a-4bc7014084a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10004\n",
      "[['text'], ['i', 'am', 'still', 'waiting', 'on', 'my', 'card']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_words = cleaning(sentences)\n",
    "print(len(cleaned_words))\n",
    "print(cleaned_words[:2])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Texts Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SJCQ_YhBJW7t"
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "  token = Tokenizer(filters = filters)\n",
    "  token.fit_on_texts(words)\n",
    "  return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJhdIJC5Q3Q6"
   },
   "outputs": [],
   "source": [
    "def max_length(words):\n",
    "  return(len(max(words, key = len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JWjxPGsZZJNX",
    "outputId": "b02c8f6b-d0df-4e90-fa3a-2ff730c88300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 2343 and Maximum length = 84\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = create_tokenizer(cleaned_words)\n",
    "vocab_size = len(word_tokenizer.word_index) + 1\n",
    "max_length = max_length(cleaned_words)\n",
    "\n",
    "print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0TXu2xsR8jq"
   },
   "outputs": [],
   "source": [
    "def encoding_doc(token, words):\n",
    "  return(token.texts_to_sequences(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dE92Hk1Va--H"
   },
   "outputs": [],
   "source": [
    "encoded_doc = encoding_doc(word_tokenizer, cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fyOzLEboc4LZ"
   },
   "outputs": [],
   "source": [
    "def padding_doc(encoded_doc, max_length):\n",
    "  return(pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WdejoJrlc-tc"
   },
   "outputs": [],
   "source": [
    "padded_doc = padding_doc(encoded_doc, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3eaSIDi0dNf1",
    "outputId": "4ab6b6dd-ffa4-4061-9e9d-7a01decfa837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded docs =  (10004, 84)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of padded docs = \",padded_doc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0rXzenSpgFR"
   },
   "outputs": [],
   "source": [
    "#tokenizer with filter changed\n",
    "output_tokenizer = create_tokenizer(unique_intent, filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "yNHQtkszskxr",
    "outputId": "f5babc01-89e3-4392-e8e6-c9f257de3d07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exchange_rate': 1,\n",
       " 'contactless_not_working': 2,\n",
       " 'declined_cash_withdrawal': 3,\n",
       " 'card_arrival': 4,\n",
       " 'card_payment_fee_charged': 5,\n",
       " 'wrong_exchange_rate_for_cash_withdrawal': 6,\n",
       " 'why_verify_identity': 7,\n",
       " 'passcode_forgotten': 8,\n",
       " 'cash_withdrawal_charge': 9,\n",
       " 'top_up_limits': 10,\n",
       " 'balance_not_updated_after_cheque_or_cash_deposit': 11,\n",
       " 'transfer_timing': 12,\n",
       " 'balance_not_updated_after_bank_transfer': 13,\n",
       " 'card_payment_not_recognised': 14,\n",
       " 'failed_transfer': 15,\n",
       " 'transaction_charged_twice': 16,\n",
       " 'order_physical_card': 17,\n",
       " 'wrong_amount_of_cash_received': 18,\n",
       " 'card_not_working': 19,\n",
       " 'pending_transfer': 20,\n",
       " 'direct_debit_payment_not_recognised': 21,\n",
       " 'getting_virtual_card': 22,\n",
       " 'edit_personal_details': 23,\n",
       " 'compromised_card': 24,\n",
       " 'transfer_fee_charged': 25,\n",
       " 'verify_my_identity': 26,\n",
       " 'country_support': 27,\n",
       " 'top_up_by_card_charge': 28,\n",
       " 'refund_not_showing_up': 29,\n",
       " 'cancel_transfer': 30,\n",
       " 'get_physical_card': 31,\n",
       " 'receiving_money': 32,\n",
       " 'card_swallowed': 33,\n",
       " 'age_limit': 34,\n",
       " 'extra_charge_on_statement': 35,\n",
       " 'disposable_card_limits': 36,\n",
       " 'change_pin': 37,\n",
       " 'declined_card_payment': 38,\n",
       " 'card_delivery_estimate': 39,\n",
       " 'reverted_card_payment': 40,\n",
       " 'card_payment_wrong_exchange_rate': 41,\n",
       " 'get_disposable_virtual_card': 42,\n",
       " 'terminate_account': 43,\n",
       " 'pending_cash_withdrawal': 44,\n",
       " 'top_up_reverted': 45,\n",
       " 'transfer_not_received_by_recipient': 46,\n",
       " 'supported_cards_and_currencies': 47,\n",
       " 'lost_or_stolen_phone': 48,\n",
       " 'category': 49,\n",
       " 'exchange_via_app': 50,\n",
       " 'atm_support': 51,\n",
       " 'pending_card_payment': 52,\n",
       " 'exchange_charge': 53,\n",
       " 'lost_or_stolen_card': 54,\n",
       " 'unable_to_verify_identity': 55,\n",
       " 'getting_spare_card': 56,\n",
       " 'virtual_card_not_working': 57,\n",
       " 'cash_withdrawal_not_recognised': 58,\n",
       " 'declined_transfer': 59,\n",
       " 'top_up_by_cash_or_cheque': 60,\n",
       " 'apple_pay_or_google_pay': 61,\n",
       " 'visa_or_mastercard': 62,\n",
       " 'beneficiary_not_allowed': 63,\n",
       " 'activate_my_card': 64,\n",
       " 'pending_top_up': 65,\n",
       " 'transfer_into_account': 66,\n",
       " 'card_about_to_expire': 67,\n",
       " 'top_up_failed': 68,\n",
       " 'pin_blocked': 69,\n",
       " 'verify_top_up': 70,\n",
       " 'request_refund': 71,\n",
       " 'card_linking': 72,\n",
       " 'automatic_top_up': 73,\n",
       " 'top_up_by_bank_transfer_charge': 74,\n",
       " 'verify_source_of_funds': 75,\n",
       " 'topping_up_by_card': 76,\n",
       " 'card_acceptance': 77,\n",
       " 'fiat_currency_support': 78}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OOx9qdBto1-"
   },
   "outputs": [],
   "source": [
    "encoded_output = encoding_doc(output_tokenizer, intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_5Lv5PiyG-z"
   },
   "outputs": [],
   "source": [
    "encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dpM86WrVQlx5",
    "outputId": "71ff52a6-b3d0-4b5c-850d-5dc0a56c8aa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10004, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rD3QN-RPzfet"
   },
   "outputs": [],
   "source": [
    "def one_hot(encode):\n",
    "  o = OneHotEncoder(sparse = False)\n",
    "  return(o.fit_transform(encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6wP_Xed7RNR"
   },
   "outputs": [],
   "source": [
    "output_one_hot = one_hot(encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A6HVslLTHgOM",
    "outputId": "752962df-02d8-409b-fb8f-adb06227161d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10004, 78)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EqABUESD7xi9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8P4HTz6A4E-"
   },
   "outputs": [],
   "source": [
    "train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7E0uhC2OCtTx",
    "outputId": "6ce0e215-aa3f-43f1-ba5a-0b584b25a35c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X = (8003, 84) and train_Y = (8003, 78)\n",
      "Shape of val_X = (2001, 84) and val_Y = (2001, 78)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n",
    "print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bidirectional GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5BU_x74DNEb"
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, max_length):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n",
    "  model.add(Bidirectional(GRU(128)))\n",
    "  model.add(Dense(32, activation = \"relu\"))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(78, activation = \"softmax\"))\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "f-NvE0P7MFCe",
    "outputId": "8f07056b-579e-4c15-e1af-bdfa8f681e79",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 84, 128)           299904    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               198144    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 78)                2574      \n",
      "=================================================================\n",
      "Total params: 508,846\n",
      "Trainable params: 208,942\n",
      "Non-trainable params: 299,904\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, max_length)\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6834
    },
    "colab_type": "code",
    "id": "_r-dxm2sMQ-d",
    "outputId": "3c37b4f8-fc4e-4c82-ab46-2aa1d8b47ffd"
   },
   "outputs": [],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 4.3465 - accuracy: 0.0151\n",
      "Epoch 00001: val_loss improved from inf to 4.32916, saving model to model.h5\n",
      "251/251 [==============================] - 19s 76ms/step - loss: 4.3465 - accuracy: 0.0151 - val_loss: 4.3292 - val_accuracy: 0.0165\n",
      "Epoch 2/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 4.2141 - accuracy: 0.0278\n",
      "Epoch 00002: val_loss improved from 4.32916 to 3.97493, saving model to model.h5\n",
      "251/251 [==============================] - 20s 80ms/step - loss: 4.2141 - accuracy: 0.0277 - val_loss: 3.9749 - val_accuracy: 0.0360\n",
      "Epoch 3/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 3.8663 - accuracy: 0.0469\n",
      "Epoch 00003: val_loss improved from 3.97493 to 3.69397, saving model to model.h5\n",
      "251/251 [==============================] - 21s 82ms/step - loss: 3.8663 - accuracy: 0.0469 - val_loss: 3.6940 - val_accuracy: 0.0925\n",
      "Epoch 4/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 3.6291 - accuracy: 0.0737\n",
      "Epoch 00004: val_loss improved from 3.69397 to 3.40917, saving model to model.h5\n",
      "251/251 [==============================] - 21s 83ms/step - loss: 3.6292 - accuracy: 0.0740 - val_loss: 3.4092 - val_accuracy: 0.1064\n",
      "Epoch 5/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 3.4454 - accuracy: 0.0886\n",
      "Epoch 00005: val_loss improved from 3.40917 to 3.13017, saving model to model.h5\n",
      "251/251 [==============================] - 21s 82ms/step - loss: 3.4451 - accuracy: 0.0886 - val_loss: 3.1302 - val_accuracy: 0.1549\n",
      "Epoch 6/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 3.2280 - accuracy: 0.1208\n",
      "Epoch 00006: val_loss improved from 3.13017 to 2.95043, saving model to model.h5\n",
      "251/251 [==============================] - 21s 82ms/step - loss: 3.2285 - accuracy: 0.1207 - val_loss: 2.9504 - val_accuracy: 0.1779\n",
      "Epoch 7/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 3.0808 - accuracy: 0.1415\n",
      "Epoch 00007: val_loss improved from 2.95043 to 2.78698, saving model to model.h5\n",
      "251/251 [==============================] - 21s 82ms/step - loss: 3.0808 - accuracy: 0.1414 - val_loss: 2.7870 - val_accuracy: 0.2299\n",
      "Epoch 8/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.9256 - accuracy: 0.1678\n",
      "Epoch 00008: val_loss improved from 2.78698 to 2.60705, saving model to model.h5\n",
      "251/251 [==============================] - 21s 83ms/step - loss: 2.9258 - accuracy: 0.1679 - val_loss: 2.6070 - val_accuracy: 0.2514\n",
      "Epoch 9/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.8068 - accuracy: 0.1916\n",
      "Epoch 00009: val_loss improved from 2.60705 to 2.50204, saving model to model.h5\n",
      "251/251 [==============================] - 23s 90ms/step - loss: 2.8068 - accuracy: 0.1917 - val_loss: 2.5020 - val_accuracy: 0.2844\n",
      "Epoch 10/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.6925 - accuracy: 0.2132\n",
      "Epoch 00010: val_loss improved from 2.50204 to 2.37907, saving model to model.h5\n",
      "251/251 [==============================] - 28s 111ms/step - loss: 2.6924 - accuracy: 0.2132 - val_loss: 2.3791 - val_accuracy: 0.3123\n",
      "Epoch 11/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.6014 - accuracy: 0.2364\n",
      "Epoch 00011: val_loss improved from 2.37907 to 2.28826, saving model to model.h5\n",
      "251/251 [==============================] - 25s 100ms/step - loss: 2.6013 - accuracy: 0.2363 - val_loss: 2.2883 - val_accuracy: 0.3293\n",
      "Epoch 12/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.5213 - accuracy: 0.2511\n",
      "Epoch 00012: val_loss improved from 2.28826 to 2.24691, saving model to model.h5\n",
      "251/251 [==============================] - 23s 92ms/step - loss: 2.5214 - accuracy: 0.2510 - val_loss: 2.2469 - val_accuracy: 0.3538\n",
      "Epoch 13/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 2.4412 - accuracy: 0.2688\n",
      "Epoch 00013: val_loss improved from 2.24691 to 2.11691, saving model to model.h5\n",
      "251/251 [==============================] - 22s 88ms/step - loss: 2.4412 - accuracy: 0.2688 - val_loss: 2.1169 - val_accuracy: 0.3823\n",
      "Epoch 14/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.3775 - accuracy: 0.2859\n",
      "Epoch 00014: val_loss improved from 2.11691 to 2.05651, saving model to model.h5\n",
      "251/251 [==============================] - 22s 87ms/step - loss: 2.3773 - accuracy: 0.2859 - val_loss: 2.0565 - val_accuracy: 0.3858\n",
      "Epoch 15/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.2898 - accuracy: 0.3054\n",
      "Epoch 00015: val_loss improved from 2.05651 to 1.99887, saving model to model.h5\n",
      "251/251 [==============================] - 22s 88ms/step - loss: 2.2893 - accuracy: 0.3055 - val_loss: 1.9989 - val_accuracy: 0.4043\n",
      "Epoch 16/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 2.2086 - accuracy: 0.3263\n",
      "Epoch 00016: val_loss improved from 1.99887 to 1.92245, saving model to model.h5\n",
      "251/251 [==============================] - 22s 90ms/step - loss: 2.2086 - accuracy: 0.3263 - val_loss: 1.9225 - val_accuracy: 0.4458\n",
      "Epoch 17/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.1518 - accuracy: 0.3280\n",
      "Epoch 00017: val_loss improved from 1.92245 to 1.88021, saving model to model.h5\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 2.1517 - accuracy: 0.3279 - val_loss: 1.8802 - val_accuracy: 0.4578\n",
      "Epoch 18/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 2.1029 - accuracy: 0.3506\n",
      "Epoch 00018: val_loss improved from 1.88021 to 1.78698, saving model to model.h5\n",
      "251/251 [==============================] - 23s 92ms/step - loss: 2.1029 - accuracy: 0.3506 - val_loss: 1.7870 - val_accuracy: 0.4728\n",
      "Epoch 19/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 2.0541 - accuracy: 0.3682\n",
      "Epoch 00019: val_loss improved from 1.78698 to 1.76506, saving model to model.h5\n",
      "251/251 [==============================] - 22s 90ms/step - loss: 2.0541 - accuracy: 0.3682 - val_loss: 1.7651 - val_accuracy: 0.4748\n",
      "Epoch 20/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.9985 - accuracy: 0.3770\n",
      "Epoch 00020: val_loss improved from 1.76506 to 1.70562, saving model to model.h5\n",
      "251/251 [==============================] - 23s 90ms/step - loss: 1.9985 - accuracy: 0.3770 - val_loss: 1.7056 - val_accuracy: 0.4873\n",
      "Epoch 21/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.9364 - accuracy: 0.3920\n",
      "Epoch 00021: val_loss improved from 1.70562 to 1.68893, saving model to model.h5\n",
      "251/251 [==============================] - 23s 91ms/step - loss: 1.9364 - accuracy: 0.3920 - val_loss: 1.6889 - val_accuracy: 0.5087\n",
      "Epoch 22/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.8991 - accuracy: 0.4047\n",
      "Epoch 00022: val_loss improved from 1.68893 to 1.62864, saving model to model.h5\n",
      "251/251 [==============================] - 23s 91ms/step - loss: 1.8991 - accuracy: 0.4047 - val_loss: 1.6286 - val_accuracy: 0.5152\n",
      "Epoch 23/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.8572 - accuracy: 0.4200\n",
      "Epoch 00023: val_loss did not improve from 1.62864\n",
      "251/251 [==============================] - 21s 85ms/step - loss: 1.8570 - accuracy: 0.4201 - val_loss: 1.6501 - val_accuracy: 0.5097\n",
      "Epoch 24/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.8172 - accuracy: 0.4344\n",
      "Epoch 00024: val_loss improved from 1.62864 to 1.60911, saving model to model.h5\n",
      "251/251 [==============================] - 21s 85ms/step - loss: 1.8175 - accuracy: 0.4342 - val_loss: 1.6091 - val_accuracy: 0.5207\n",
      "Epoch 25/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.7977 - accuracy: 0.4329\n",
      "Epoch 00025: val_loss improved from 1.60911 to 1.60057, saving model to model.h5\n",
      "251/251 [==============================] - 21s 85ms/step - loss: 1.7977 - accuracy: 0.4328 - val_loss: 1.6006 - val_accuracy: 0.5267\n",
      "Epoch 26/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.7604 - accuracy: 0.4426\n",
      "Epoch 00026: val_loss improved from 1.60057 to 1.53701, saving model to model.h5\n",
      "251/251 [==============================] - 21s 85ms/step - loss: 1.7608 - accuracy: 0.4426 - val_loss: 1.5370 - val_accuracy: 0.5422\n",
      "Epoch 27/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.6923 - accuracy: 0.4602\n",
      "Epoch 00027: val_loss did not improve from 1.53701\n",
      "251/251 [==============================] - 21s 85ms/step - loss: 1.6925 - accuracy: 0.4602 - val_loss: 1.5382 - val_accuracy: 0.5362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.6912 - accuracy: 0.4611\n",
      "Epoch 00028: val_loss improved from 1.53701 to 1.50611, saving model to model.h5\n",
      "251/251 [==============================] - 21s 82ms/step - loss: 1.6908 - accuracy: 0.4612 - val_loss: 1.5061 - val_accuracy: 0.5587\n",
      "Epoch 29/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.6325 - accuracy: 0.4681\n",
      "Epoch 00029: val_loss improved from 1.50611 to 1.49190, saving model to model.h5\n",
      "251/251 [==============================] - 22s 87ms/step - loss: 1.6325 - accuracy: 0.4681 - val_loss: 1.4919 - val_accuracy: 0.5662\n",
      "Epoch 30/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.6268 - accuracy: 0.4768\n",
      "Epoch 00030: val_loss improved from 1.49190 to 1.47826, saving model to model.h5\n",
      "251/251 [==============================] - 25s 98ms/step - loss: 1.6268 - accuracy: 0.4768 - val_loss: 1.4783 - val_accuracy: 0.5707\n",
      "Epoch 31/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.5913 - accuracy: 0.4861\n",
      "Epoch 00031: val_loss improved from 1.47826 to 1.45580, saving model to model.h5\n",
      "251/251 [==============================] - 22s 88ms/step - loss: 1.5916 - accuracy: 0.4861 - val_loss: 1.4558 - val_accuracy: 0.5752\n",
      "Epoch 32/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.5683 - accuracy: 0.4960\n",
      "Epoch 00032: val_loss improved from 1.45580 to 1.43756, saving model to model.h5\n",
      "251/251 [==============================] - 24s 94ms/step - loss: 1.5687 - accuracy: 0.4958 - val_loss: 1.4376 - val_accuracy: 0.5817\n",
      "Epoch 33/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.5258 - accuracy: 0.5102\n",
      "Epoch 00033: val_loss did not improve from 1.43756\n",
      "251/251 [==============================] - 21s 84ms/step - loss: 1.5258 - accuracy: 0.5102 - val_loss: 1.4432 - val_accuracy: 0.5837\n",
      "Epoch 34/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.4766 - accuracy: 0.5246\n",
      "Epoch 00034: val_loss did not improve from 1.43756\n",
      "251/251 [==============================] - 20s 80ms/step - loss: 1.4767 - accuracy: 0.5246 - val_loss: 1.4625 - val_accuracy: 0.5837\n",
      "Epoch 35/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.4459 - accuracy: 0.5350\n",
      "Epoch 00035: val_loss improved from 1.43756 to 1.43671, saving model to model.h5\n",
      "251/251 [==============================] - 19s 74ms/step - loss: 1.4461 - accuracy: 0.5349 - val_loss: 1.4367 - val_accuracy: 0.5862\n",
      "Epoch 36/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.4073 - accuracy: 0.5430\n",
      "Epoch 00036: val_loss improved from 1.43671 to 1.42978, saving model to model.h5\n",
      "251/251 [==============================] - 22s 88ms/step - loss: 1.4073 - accuracy: 0.5430 - val_loss: 1.4298 - val_accuracy: 0.5907\n",
      "Epoch 37/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.3977 - accuracy: 0.5452\n",
      "Epoch 00037: val_loss improved from 1.42978 to 1.41595, saving model to model.h5\n",
      "251/251 [==============================] - 26s 105ms/step - loss: 1.3976 - accuracy: 0.5453 - val_loss: 1.4159 - val_accuracy: 0.6037\n",
      "Epoch 38/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.3391 - accuracy: 0.5569\n",
      "Epoch 00038: val_loss improved from 1.41595 to 1.38341, saving model to model.h5\n",
      "251/251 [==============================] - 25s 101ms/step - loss: 1.3391 - accuracy: 0.5569 - val_loss: 1.3834 - val_accuracy: 0.6032\n",
      "Epoch 39/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.3228 - accuracy: 0.5642\n",
      "Epoch 00039: val_loss improved from 1.38341 to 1.36265, saving model to model.h5\n",
      "251/251 [==============================] - 27s 106ms/step - loss: 1.3224 - accuracy: 0.5644 - val_loss: 1.3626 - val_accuracy: 0.6192\n",
      "Epoch 40/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.3018 - accuracy: 0.5720\n",
      "Epoch 00040: val_loss improved from 1.36265 to 1.36014, saving model to model.h5\n",
      "251/251 [==============================] - 27s 106ms/step - loss: 1.3018 - accuracy: 0.5720 - val_loss: 1.3601 - val_accuracy: 0.6277\n",
      "Epoch 41/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.2914 - accuracy: 0.5819\n",
      "Epoch 00041: val_loss improved from 1.36014 to 1.35651, saving model to model.h5\n",
      "251/251 [==============================] - 25s 99ms/step - loss: 1.2911 - accuracy: 0.5819 - val_loss: 1.3565 - val_accuracy: 0.6217\n",
      "Epoch 42/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.2503 - accuracy: 0.5850\n",
      "Epoch 00042: val_loss did not improve from 1.35651\n",
      "251/251 [==============================] - 24s 96ms/step - loss: 1.2503 - accuracy: 0.5850 - val_loss: 1.3832 - val_accuracy: 0.6327\n",
      "Epoch 43/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.2336 - accuracy: 0.5953\n",
      "Epoch 00043: val_loss improved from 1.35651 to 1.35074, saving model to model.h5\n",
      "251/251 [==============================] - 26s 102ms/step - loss: 1.2334 - accuracy: 0.5953 - val_loss: 1.3507 - val_accuracy: 0.6372\n",
      "Epoch 44/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.1979 - accuracy: 0.5960\n",
      "Epoch 00044: val_loss did not improve from 1.35074\n",
      "251/251 [==============================] - 19s 76ms/step - loss: 1.1978 - accuracy: 0.5960 - val_loss: 1.4410 - val_accuracy: 0.6217\n",
      "Epoch 45/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.2629 - accuracy: 0.5861\n",
      "Epoch 00045: val_loss improved from 1.35074 to 1.34681, saving model to model.h5\n",
      "251/251 [==============================] - 18s 73ms/step - loss: 1.2628 - accuracy: 0.5862 - val_loss: 1.3468 - val_accuracy: 0.6292\n",
      "Epoch 46/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.1725 - accuracy: 0.6116\n",
      "Epoch 00046: val_loss did not improve from 1.34681\n",
      "251/251 [==============================] - 18s 73ms/step - loss: 1.1721 - accuracy: 0.6118 - val_loss: 1.3866 - val_accuracy: 0.6297\n",
      "Epoch 47/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.1513 - accuracy: 0.6162\n",
      "Epoch 00047: val_loss improved from 1.34681 to 1.34363, saving model to model.h5\n",
      "251/251 [==============================] - 22s 87ms/step - loss: 1.1514 - accuracy: 0.6163 - val_loss: 1.3436 - val_accuracy: 0.6497\n",
      "Epoch 48/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.0900 - accuracy: 0.6400\n",
      "Epoch 00048: val_loss did not improve from 1.34363\n",
      "251/251 [==============================] - 25s 99ms/step - loss: 1.0913 - accuracy: 0.6399 - val_loss: 1.4173 - val_accuracy: 0.6337\n",
      "Epoch 49/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.1320 - accuracy: 0.6271\n",
      "Epoch 00049: val_loss improved from 1.34363 to 1.33687, saving model to model.h5\n",
      "251/251 [==============================] - 26s 104ms/step - loss: 1.1320 - accuracy: 0.6270 - val_loss: 1.3369 - val_accuracy: 0.6442\n",
      "Epoch 50/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.0646 - accuracy: 0.6410\n",
      "Epoch 00050: val_loss did not improve from 1.33687\n",
      "251/251 [==============================] - 25s 102ms/step - loss: 1.0645 - accuracy: 0.6411 - val_loss: 1.4024 - val_accuracy: 0.6562\n",
      "Epoch 51/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.0427 - accuracy: 0.6486\n",
      "Epoch 00051: val_loss did not improve from 1.33687\n",
      "251/251 [==============================] - 20s 81ms/step - loss: 1.0425 - accuracy: 0.6486 - val_loss: 1.3860 - val_accuracy: 0.6582\n",
      "Epoch 52/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.0454 - accuracy: 0.6469\n",
      "Epoch 00052: val_loss did not improve from 1.33687\n",
      "251/251 [==============================] - 19s 76ms/step - loss: 1.0455 - accuracy: 0.6468 - val_loss: 1.3690 - val_accuracy: 0.6462\n",
      "Epoch 53/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.0017 - accuracy: 0.6579\n",
      "Epoch 00053: val_loss did not improve from 1.33687\n",
      "251/251 [==============================] - 20s 78ms/step - loss: 1.0017 - accuracy: 0.6579 - val_loss: 1.3882 - val_accuracy: 0.6627\n",
      "Epoch 54/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9773 - accuracy: 0.6711\n",
      "Epoch 00054: val_loss improved from 1.33687 to 1.32746, saving model to model.h5\n",
      "251/251 [==============================] - 19s 74ms/step - loss: 0.9777 - accuracy: 0.6709 - val_loss: 1.3275 - val_accuracy: 0.6762\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/251 [============================>.] - ETA: 0s - loss: 1.0022 - accuracy: 0.6619\n",
      "Epoch 00055: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 19s 77ms/step - loss: 1.0026 - accuracy: 0.6618 - val_loss: 1.4188 - val_accuracy: 0.6622\n",
      "Epoch 56/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9672 - accuracy: 0.6740\n",
      "Epoch 00056: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 22s 87ms/step - loss: 0.9670 - accuracy: 0.6741 - val_loss: 1.4549 - val_accuracy: 0.6647\n",
      "Epoch 57/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9549 - accuracy: 0.6837\n",
      "Epoch 00057: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 19s 75ms/step - loss: 0.9546 - accuracy: 0.6839 - val_loss: 1.4007 - val_accuracy: 0.6667\n",
      "Epoch 58/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9149 - accuracy: 0.6886\n",
      "Epoch 00058: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 19s 76ms/step - loss: 0.9150 - accuracy: 0.6887 - val_loss: 1.4111 - val_accuracy: 0.6642\n",
      "Epoch 59/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9581 - accuracy: 0.6764\n",
      "Epoch 00059: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 20s 79ms/step - loss: 0.9581 - accuracy: 0.6764 - val_loss: 1.4775 - val_accuracy: 0.6617\n",
      "Epoch 60/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9024 - accuracy: 0.6906\n",
      "Epoch 00060: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 20s 78ms/step - loss: 0.9030 - accuracy: 0.6905 - val_loss: 1.4273 - val_accuracy: 0.6597\n",
      "Epoch 61/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.8618 - accuracy: 0.7085\n",
      "Epoch 00061: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 20s 79ms/step - loss: 0.8621 - accuracy: 0.7084 - val_loss: 1.4666 - val_accuracy: 0.6607\n",
      "Epoch 62/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.8658 - accuracy: 0.7032\n",
      "Epoch 00062: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 19s 77ms/step - loss: 0.8656 - accuracy: 0.7032 - val_loss: 1.6600 - val_accuracy: 0.6612\n",
      "Epoch 63/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.8625 - accuracy: 0.7040\n",
      "Epoch 00063: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 25s 99ms/step - loss: 0.8625 - accuracy: 0.7040 - val_loss: 1.5090 - val_accuracy: 0.6782\n",
      "Epoch 64/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.8235 - accuracy: 0.7124\n",
      "Epoch 00064: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 21s 85ms/step - loss: 0.8236 - accuracy: 0.7124 - val_loss: 1.4528 - val_accuracy: 0.6822\n",
      "Epoch 65/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.8525 - accuracy: 0.7072\n",
      "Epoch 00065: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 20s 78ms/step - loss: 0.8529 - accuracy: 0.7071 - val_loss: 1.4178 - val_accuracy: 0.6847\n",
      "Epoch 66/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.8196 - accuracy: 0.7134\n",
      "Epoch 00066: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 22s 86ms/step - loss: 0.8200 - accuracy: 0.7134 - val_loss: 1.5517 - val_accuracy: 0.6637\n",
      "Epoch 67/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.8137 - accuracy: 0.7209\n",
      "Epoch 00067: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 23s 92ms/step - loss: 0.8137 - accuracy: 0.7209 - val_loss: 1.5501 - val_accuracy: 0.6722\n",
      "Epoch 68/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.8089 - accuracy: 0.7287\n",
      "Epoch 00068: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 26s 105ms/step - loss: 0.8089 - accuracy: 0.7287 - val_loss: 1.4992 - val_accuracy: 0.6902\n",
      "Epoch 69/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.8299 - accuracy: 0.7132\n",
      "Epoch 00069: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 22s 86ms/step - loss: 0.8297 - accuracy: 0.7134 - val_loss: 1.5369 - val_accuracy: 0.6722\n",
      "Epoch 70/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.7657 - accuracy: 0.7352\n",
      "Epoch 00070: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 25s 98ms/step - loss: 0.7663 - accuracy: 0.7350 - val_loss: 1.6164 - val_accuracy: 0.6697\n",
      "Epoch 71/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.7516 - accuracy: 0.7385\n",
      "Epoch 00071: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 27s 106ms/step - loss: 0.7516 - accuracy: 0.7385 - val_loss: 1.5888 - val_accuracy: 0.6822\n",
      "Epoch 72/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.7486 - accuracy: 0.7365\n",
      "Epoch 00072: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 28s 110ms/step - loss: 0.7484 - accuracy: 0.7366 - val_loss: 1.6463 - val_accuracy: 0.6802\n",
      "Epoch 73/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.7099 - accuracy: 0.7459\n",
      "Epoch 00073: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 26s 104ms/step - loss: 0.7098 - accuracy: 0.7460 - val_loss: 1.7012 - val_accuracy: 0.6797\n",
      "Epoch 74/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.7023 - accuracy: 0.7506\n",
      "Epoch 00074: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 26s 106ms/step - loss: 0.7023 - accuracy: 0.7506 - val_loss: 1.5766 - val_accuracy: 0.6952\n",
      "Epoch 75/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.7154 - accuracy: 0.7524\n",
      "Epoch 00075: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 27s 108ms/step - loss: 0.7154 - accuracy: 0.7523 - val_loss: 1.6799 - val_accuracy: 0.6757\n",
      "Epoch 76/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.7062 - accuracy: 0.7485\n",
      "Epoch 00076: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 26s 105ms/step - loss: 0.7071 - accuracy: 0.7485 - val_loss: 1.6368 - val_accuracy: 0.6872\n",
      "Epoch 77/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.7022 - accuracy: 0.7567\n",
      "Epoch 00077: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 26s 105ms/step - loss: 0.7020 - accuracy: 0.7568 - val_loss: 1.6156 - val_accuracy: 0.6907\n",
      "Epoch 78/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.7189 - accuracy: 0.7516\n",
      "Epoch 00078: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 21s 82ms/step - loss: 0.7190 - accuracy: 0.7516 - val_loss: 1.6409 - val_accuracy: 0.6902\n",
      "Epoch 79/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.6533 - accuracy: 0.7671\n",
      "Epoch 00079: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 23s 94ms/step - loss: 0.6533 - accuracy: 0.7671 - val_loss: 1.7540 - val_accuracy: 0.6922\n",
      "Epoch 80/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.6888 - accuracy: 0.7565\n",
      "Epoch 00080: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 29s 117ms/step - loss: 0.6888 - accuracy: 0.7565 - val_loss: 1.7741 - val_accuracy: 0.6817\n",
      "Epoch 81/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.6462 - accuracy: 0.7692\n",
      "Epoch 00081: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 25s 100ms/step - loss: 0.6460 - accuracy: 0.7693 - val_loss: 1.7142 - val_accuracy: 0.6897\n",
      "Epoch 82/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.6398 - accuracy: 0.7757\n",
      "Epoch 00082: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 25s 98ms/step - loss: 0.6397 - accuracy: 0.7758 - val_loss: 1.7746 - val_accuracy: 0.6842\n",
      "Epoch 83/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.6153 - accuracy: 0.7815\n",
      "Epoch 00083: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 25s 101ms/step - loss: 0.6153 - accuracy: 0.7815 - val_loss: 1.7445 - val_accuracy: 0.6917\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/251 [==============================] - ETA: 0s - loss: 0.6376 - accuracy: 0.7806\n",
      "Epoch 00084: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 28s 113ms/step - loss: 0.6376 - accuracy: 0.7806 - val_loss: 1.7580 - val_accuracy: 0.6887\n",
      "Epoch 85/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.6081 - accuracy: 0.7836\n",
      "Epoch 00085: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 20s 79ms/step - loss: 0.6084 - accuracy: 0.7835 - val_loss: 1.7240 - val_accuracy: 0.7096\n",
      "Epoch 86/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.6152 - accuracy: 0.7806\n",
      "Epoch 00086: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 18s 72ms/step - loss: 0.6150 - accuracy: 0.7807 - val_loss: 1.9603 - val_accuracy: 0.6827\n",
      "Epoch 87/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.6544 - accuracy: 0.7774\n",
      "Epoch 00087: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 18s 72ms/step - loss: 0.6548 - accuracy: 0.7772 - val_loss: 1.7448 - val_accuracy: 0.6887\n",
      "Epoch 88/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.6127 - accuracy: 0.7820\n",
      "Epoch 00088: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 18s 72ms/step - loss: 0.6125 - accuracy: 0.7821 - val_loss: 1.8716 - val_accuracy: 0.6877\n",
      "Epoch 89/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.6193 - accuracy: 0.7830\n",
      "Epoch 00089: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 18s 72ms/step - loss: 0.6194 - accuracy: 0.7830 - val_loss: 1.9035 - val_accuracy: 0.6992\n",
      "Epoch 90/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.5926 - accuracy: 0.7928\n",
      "Epoch 00090: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 18s 73ms/step - loss: 0.5924 - accuracy: 0.7928 - val_loss: 1.8075 - val_accuracy: 0.6917\n",
      "Epoch 91/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.5663 - accuracy: 0.7993\n",
      "Epoch 00091: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 18s 72ms/step - loss: 0.5665 - accuracy: 0.7992 - val_loss: 1.9050 - val_accuracy: 0.6982\n",
      "Epoch 92/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.5705 - accuracy: 0.8087\n",
      "Epoch 00092: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 27s 108ms/step - loss: 0.5705 - accuracy: 0.8087 - val_loss: 1.9498 - val_accuracy: 0.6957\n",
      "Epoch 93/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.5548 - accuracy: 0.8031\n",
      "Epoch 00093: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 24s 96ms/step - loss: 0.5549 - accuracy: 0.8031 - val_loss: 1.8895 - val_accuracy: 0.6987\n",
      "Epoch 94/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.6028 - accuracy: 0.7861\n",
      "Epoch 00094: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 18s 73ms/step - loss: 0.6027 - accuracy: 0.7861 - val_loss: 1.9240 - val_accuracy: 0.6822\n",
      "Epoch 95/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.5820 - accuracy: 0.7975\n",
      "Epoch 00095: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 18s 73ms/step - loss: 0.5821 - accuracy: 0.7973 - val_loss: 1.9059 - val_accuracy: 0.6907\n",
      "Epoch 96/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.5321 - accuracy: 0.8110\n",
      "Epoch 00096: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 18s 73ms/step - loss: 0.5320 - accuracy: 0.8111 - val_loss: 1.9763 - val_accuracy: 0.6947\n",
      "Epoch 97/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.5366 - accuracy: 0.8096\n",
      "Epoch 00097: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 18s 73ms/step - loss: 0.5364 - accuracy: 0.8097 - val_loss: 1.9246 - val_accuracy: 0.6952\n",
      "Epoch 98/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.5715 - accuracy: 0.8070\n",
      "Epoch 00098: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 18s 73ms/step - loss: 0.5714 - accuracy: 0.8069 - val_loss: 1.9964 - val_accuracy: 0.6862\n",
      "Epoch 99/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.5184 - accuracy: 0.8174\n",
      "Epoch 00099: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 18s 73ms/step - loss: 0.5183 - accuracy: 0.8174 - val_loss: 2.1322 - val_accuracy: 0.7031\n",
      "Epoch 100/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.5081 - accuracy: 0.8186\n",
      "Epoch 00100: val_loss did not improve from 1.32746\n",
      "251/251 [==============================] - 19s 75ms/step - loss: 0.5082 - accuracy: 0.8186 - val_loss: 1.9790 - val_accuracy: 0.6952\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_X, train_Y,\n",
    "                 epochs = 100,\n",
    "                 batch_size = 32,\n",
    "                 validation_data = (val_X, val_Y),\n",
    "                 callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bidirectional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 84, 128)           299904    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 78)                2574      \n",
      "=================================================================\n",
      "Total params: 573,870\n",
      "Trainable params: 273,966\n",
      "Non-trainable params: 299,904\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(vocab_size, max_length):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n",
    "  model.add(Bidirectional(LSTM(128)))\n",
    "  model.add(Dense(32, activation = \"relu\"))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(78, activation = \"softmax\"))\n",
    "  \n",
    "  return model\n",
    "\n",
    "model_lstm = create_model(vocab_size, max_length)\n",
    "\n",
    "model_lstm.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 4.3283 - accuracy: 0.0184\n",
      "Epoch 00001: val_loss improved from inf to 4.23865, saving model to model.h5\n",
      "251/251 [==============================] - 23s 92ms/step - loss: 4.3278 - accuracy: 0.0184 - val_loss: 4.2386 - val_accuracy: 0.0150\n",
      "Epoch 2/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 4.1311 - accuracy: 0.0318\n",
      "Epoch 00002: val_loss improved from 4.23865 to 3.97603, saving model to model.h5\n",
      "251/251 [==============================] - 23s 90ms/step - loss: 4.1311 - accuracy: 0.0317 - val_loss: 3.9760 - val_accuracy: 0.0500\n",
      "Epoch 3/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 3.8447 - accuracy: 0.0582\n",
      "Epoch 00003: val_loss improved from 3.97603 to 3.59888, saving model to model.h5\n",
      "251/251 [==============================] - 23s 90ms/step - loss: 3.8447 - accuracy: 0.0582 - val_loss: 3.5989 - val_accuracy: 0.0815\n",
      "Epoch 4/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 3.5645 - accuracy: 0.0911\n",
      "Epoch 00004: val_loss improved from 3.59888 to 3.33815, saving model to model.h5\n",
      "251/251 [==============================] - 23s 90ms/step - loss: 3.5644 - accuracy: 0.0911 - val_loss: 3.3381 - val_accuracy: 0.1119\n",
      "Epoch 5/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 3.3655 - accuracy: 0.1091\n",
      "Epoch 00005: val_loss improved from 3.33815 to 3.15023, saving model to model.h5\n",
      "251/251 [==============================] - 23s 90ms/step - loss: 3.3658 - accuracy: 0.1091 - val_loss: 3.1502 - val_accuracy: 0.1509\n",
      "Epoch 6/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 3.1973 - accuracy: 0.1281\n",
      "Epoch 00006: val_loss improved from 3.15023 to 2.97285, saving model to model.h5\n",
      "251/251 [==============================] - 23s 90ms/step - loss: 3.1969 - accuracy: 0.1281 - val_loss: 2.9729 - val_accuracy: 0.1669\n",
      "Epoch 7/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 3.0852 - accuracy: 0.1417\n",
      "Epoch 00007: val_loss improved from 2.97285 to 2.87104, saving model to model.h5\n",
      "251/251 [==============================] - 23s 90ms/step - loss: 3.0852 - accuracy: 0.1417 - val_loss: 2.8710 - val_accuracy: 0.1954\n",
      "Epoch 8/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.9413 - accuracy: 0.1628\n",
      "Epoch 00008: val_loss improved from 2.87104 to 2.74773, saving model to model.h5\n",
      "251/251 [==============================] - 23s 90ms/step - loss: 2.9414 - accuracy: 0.1627 - val_loss: 2.7477 - val_accuracy: 0.2079\n",
      "Epoch 9/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.8181 - accuracy: 0.1902\n",
      "Epoch 00009: val_loss improved from 2.74773 to 2.61223, saving model to model.h5\n",
      "251/251 [==============================] - 24s 96ms/step - loss: 2.8184 - accuracy: 0.1902 - val_loss: 2.6122 - val_accuracy: 0.2414\n",
      "Epoch 10/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.7171 - accuracy: 0.2114\n",
      "Epoch 00010: val_loss improved from 2.61223 to 2.45857, saving model to model.h5\n",
      "251/251 [==============================] - 23s 90ms/step - loss: 2.7174 - accuracy: 0.2114 - val_loss: 2.4586 - val_accuracy: 0.2869\n",
      "Epoch 11/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.6902 - accuracy: 0.2139\n",
      "Epoch 00011: val_loss improved from 2.45857 to 2.42147, saving model to model.h5\n",
      "251/251 [==============================] - 24s 96ms/step - loss: 2.6899 - accuracy: 0.2140 - val_loss: 2.4215 - val_accuracy: 0.2854\n",
      "Epoch 12/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.5676 - accuracy: 0.2368\n",
      "Epoch 00012: val_loss improved from 2.42147 to 2.35615, saving model to model.h5\n",
      "251/251 [==============================] - 26s 104ms/step - loss: 2.5677 - accuracy: 0.2367 - val_loss: 2.3562 - val_accuracy: 0.3063\n",
      "Epoch 13/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.4719 - accuracy: 0.2625\n",
      "Epoch 00013: val_loss improved from 2.35615 to 2.28385, saving model to model.h5\n",
      "251/251 [==============================] - 32s 129ms/step - loss: 2.4718 - accuracy: 0.2625 - val_loss: 2.2839 - val_accuracy: 0.3283\n",
      "Epoch 14/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.4045 - accuracy: 0.2765\n",
      "Epoch 00014: val_loss improved from 2.28385 to 2.20581, saving model to model.h5\n",
      "251/251 [==============================] - 31s 122ms/step - loss: 2.4043 - accuracy: 0.2765 - val_loss: 2.2058 - val_accuracy: 0.3313\n",
      "Epoch 15/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 2.3567 - accuracy: 0.2840\n",
      "Epoch 00015: val_loss did not improve from 2.20581\n",
      "251/251 [==============================] - 45s 178ms/step - loss: 2.3567 - accuracy: 0.2840 - val_loss: 2.2380 - val_accuracy: 0.3513\n",
      "Epoch 16/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.3286 - accuracy: 0.2924\n",
      "Epoch 00016: val_loss improved from 2.20581 to 2.09581, saving model to model.h5\n",
      "251/251 [==============================] - 27s 108ms/step - loss: 2.3287 - accuracy: 0.2925 - val_loss: 2.0958 - val_accuracy: 0.3898\n",
      "Epoch 17/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.2494 - accuracy: 0.3121\n",
      "Epoch 00017: val_loss improved from 2.09581 to 2.05365, saving model to model.h5\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 2.2496 - accuracy: 0.3120 - val_loss: 2.0536 - val_accuracy: 0.3913\n",
      "Epoch 18/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.1781 - accuracy: 0.3237\n",
      "Epoch 00018: val_loss improved from 2.05365 to 1.98880, saving model to model.h5\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 2.1781 - accuracy: 0.3238 - val_loss: 1.9888 - val_accuracy: 0.4168\n",
      "Epoch 19/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.1501 - accuracy: 0.3338\n",
      "Epoch 00019: val_loss improved from 1.98880 to 1.94579, saving model to model.h5\n",
      "251/251 [==============================] - 24s 94ms/step - loss: 2.1497 - accuracy: 0.3339 - val_loss: 1.9458 - val_accuracy: 0.4293\n",
      "Epoch 20/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.0849 - accuracy: 0.3528\n",
      "Epoch 00020: val_loss improved from 1.94579 to 1.88991, saving model to model.h5\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 2.0847 - accuracy: 0.3529 - val_loss: 1.8899 - val_accuracy: 0.4363\n",
      "Epoch 21/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 2.0381 - accuracy: 0.3719\n",
      "Epoch 00021: val_loss improved from 1.88991 to 1.84056, saving model to model.h5\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 2.0384 - accuracy: 0.3717 - val_loss: 1.8406 - val_accuracy: 0.4543\n",
      "Epoch 22/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.9939 - accuracy: 0.3829\n",
      "Epoch 00022: val_loss did not improve from 1.84056\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 1.9935 - accuracy: 0.3830 - val_loss: 1.8624 - val_accuracy: 0.4548\n",
      "Epoch 23/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.9408 - accuracy: 0.3981\n",
      "Epoch 00023: val_loss did not improve from 1.84056\n",
      "251/251 [==============================] - 24s 95ms/step - loss: 1.9412 - accuracy: 0.3982 - val_loss: 1.8515 - val_accuracy: 0.4613\n",
      "Epoch 24/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.8984 - accuracy: 0.4086\n",
      "Epoch 00024: val_loss improved from 1.84056 to 1.75547, saving model to model.h5\n",
      "251/251 [==============================] - 27s 109ms/step - loss: 1.8981 - accuracy: 0.4087 - val_loss: 1.7555 - val_accuracy: 0.4868\n",
      "Epoch 25/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.8770 - accuracy: 0.4198\n",
      "Epoch 00025: val_loss improved from 1.75547 to 1.73056, saving model to model.h5\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 1.8784 - accuracy: 0.4197 - val_loss: 1.7306 - val_accuracy: 0.4953\n",
      "Epoch 26/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.8316 - accuracy: 0.4226\n",
      "Epoch 00026: val_loss improved from 1.73056 to 1.71445, saving model to model.h5\n",
      "251/251 [==============================] - 25s 101ms/step - loss: 1.8316 - accuracy: 0.4226 - val_loss: 1.7144 - val_accuracy: 0.5002\n",
      "Epoch 27/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.8079 - accuracy: 0.4286\n",
      "Epoch 00027: val_loss improved from 1.71445 to 1.67843, saving model to model.h5\n",
      "251/251 [==============================] - 35s 141ms/step - loss: 1.8083 - accuracy: 0.4286 - val_loss: 1.6784 - val_accuracy: 0.5127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.7536 - accuracy: 0.4535\n",
      "Epoch 00028: val_loss did not improve from 1.67843\n",
      "251/251 [==============================] - 28s 110ms/step - loss: 1.7535 - accuracy: 0.4536 - val_loss: 1.7034 - val_accuracy: 0.4988\n",
      "Epoch 29/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.7234 - accuracy: 0.4550\n",
      "Epoch 00029: val_loss did not improve from 1.67843\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 1.7238 - accuracy: 0.4550 - val_loss: 1.7211 - val_accuracy: 0.5042\n",
      "Epoch 30/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.8046 - accuracy: 0.4397\n",
      "Epoch 00030: val_loss improved from 1.67843 to 1.65198, saving model to model.h5\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 1.8046 - accuracy: 0.4397 - val_loss: 1.6520 - val_accuracy: 0.5092\n",
      "Epoch 31/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.6849 - accuracy: 0.4712\n",
      "Epoch 00031: val_loss improved from 1.65198 to 1.64578, saving model to model.h5\n",
      "251/251 [==============================] - 29s 114ms/step - loss: 1.6849 - accuracy: 0.4712 - val_loss: 1.6458 - val_accuracy: 0.5392\n",
      "Epoch 32/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.6350 - accuracy: 0.4849\n",
      "Epoch 00032: val_loss improved from 1.64578 to 1.59040, saving model to model.h5\n",
      "251/251 [==============================] - 41s 163ms/step - loss: 1.6350 - accuracy: 0.4849 - val_loss: 1.5904 - val_accuracy: 0.5382\n",
      "Epoch 33/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.6103 - accuracy: 0.4923\n",
      "Epoch 00033: val_loss improved from 1.59040 to 1.56940, saving model to model.h5\n",
      "251/251 [==============================] - 42s 166ms/step - loss: 1.6103 - accuracy: 0.4923 - val_loss: 1.5694 - val_accuracy: 0.5457\n",
      "Epoch 34/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.5683 - accuracy: 0.5051\n",
      "Epoch 00034: val_loss did not improve from 1.56940\n",
      "251/251 [==============================] - 41s 165ms/step - loss: 1.5683 - accuracy: 0.5051 - val_loss: 1.6619 - val_accuracy: 0.5217\n",
      "Epoch 35/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.5470 - accuracy: 0.5086\n",
      "Epoch 00035: val_loss did not improve from 1.56940\n",
      "251/251 [==============================] - 42s 166ms/step - loss: 1.5470 - accuracy: 0.5086 - val_loss: 1.5719 - val_accuracy: 0.5507\n",
      "Epoch 36/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.5344 - accuracy: 0.5242\n",
      "Epoch 00036: val_loss did not improve from 1.56940\n",
      "251/251 [==============================] - 42s 166ms/step - loss: 1.5345 - accuracy: 0.5241 - val_loss: 1.6119 - val_accuracy: 0.5492\n",
      "Epoch 37/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.5130 - accuracy: 0.5177\n",
      "Epoch 00037: val_loss improved from 1.56940 to 1.54923, saving model to model.h5\n",
      "251/251 [==============================] - 42s 168ms/step - loss: 1.5130 - accuracy: 0.5177 - val_loss: 1.5492 - val_accuracy: 0.5582\n",
      "Epoch 38/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.4740 - accuracy: 0.5348\n",
      "Epoch 00038: val_loss improved from 1.54923 to 1.54885, saving model to model.h5\n",
      "251/251 [==============================] - 43s 170ms/step - loss: 1.4740 - accuracy: 0.5348 - val_loss: 1.5489 - val_accuracy: 0.5617\n",
      "Epoch 39/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.4878 - accuracy: 0.5281\n",
      "Epoch 00039: val_loss did not improve from 1.54885\n",
      "251/251 [==============================] - 42s 168ms/step - loss: 1.4878 - accuracy: 0.5281 - val_loss: 1.6041 - val_accuracy: 0.5597\n",
      "Epoch 40/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.4336 - accuracy: 0.5406\n",
      "Epoch 00040: val_loss improved from 1.54885 to 1.52225, saving model to model.h5\n",
      "251/251 [==============================] - 41s 162ms/step - loss: 1.4337 - accuracy: 0.5407 - val_loss: 1.5223 - val_accuracy: 0.5737\n",
      "Epoch 41/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.4077 - accuracy: 0.5418\n",
      "Epoch 00041: val_loss did not improve from 1.52225\n",
      "251/251 [==============================] - 41s 161ms/step - loss: 1.4077 - accuracy: 0.5418 - val_loss: 1.5434 - val_accuracy: 0.5857\n",
      "Epoch 42/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.3933 - accuracy: 0.5474\n",
      "Epoch 00042: val_loss did not improve from 1.52225\n",
      "251/251 [==============================] - 41s 163ms/step - loss: 1.3933 - accuracy: 0.5474 - val_loss: 1.5599 - val_accuracy: 0.5657\n",
      "Epoch 43/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.3324 - accuracy: 0.5700\n",
      "Epoch 00043: val_loss did not improve from 1.52225\n",
      "251/251 [==============================] - 42s 167ms/step - loss: 1.3324 - accuracy: 0.5700 - val_loss: 1.5838 - val_accuracy: 0.5692\n",
      "Epoch 44/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.3168 - accuracy: 0.5779\n",
      "Epoch 00044: val_loss did not improve from 1.52225\n",
      "251/251 [==============================] - 39s 155ms/step - loss: 1.3168 - accuracy: 0.5779 - val_loss: 1.5509 - val_accuracy: 0.5982\n",
      "Epoch 45/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.3256 - accuracy: 0.5770\n",
      "Epoch 00045: val_loss did not improve from 1.52225\n",
      "251/251 [==============================] - 37s 147ms/step - loss: 1.3256 - accuracy: 0.5770 - val_loss: 1.5309 - val_accuracy: 0.5872\n",
      "Epoch 46/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.2833 - accuracy: 0.5788\n",
      "Epoch 00046: val_loss did not improve from 1.52225\n",
      "251/251 [==============================] - 42s 168ms/step - loss: 1.2833 - accuracy: 0.5788 - val_loss: 1.5325 - val_accuracy: 0.5877\n",
      "Epoch 47/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.2616 - accuracy: 0.5885\n",
      "Epoch 00047: val_loss did not improve from 1.52225\n",
      "251/251 [==============================] - 41s 162ms/step - loss: 1.2616 - accuracy: 0.5885 - val_loss: 1.5313 - val_accuracy: 0.5932\n",
      "Epoch 48/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.2828 - accuracy: 0.5842\n",
      "Epoch 00048: val_loss improved from 1.52225 to 1.47513, saving model to model.h5\n",
      "251/251 [==============================] - 26s 102ms/step - loss: 1.2828 - accuracy: 0.5842 - val_loss: 1.4751 - val_accuracy: 0.5962\n",
      "Epoch 49/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.2431 - accuracy: 0.5975\n",
      "Epoch 00049: val_loss did not improve from 1.47513\n",
      "251/251 [==============================] - 23s 92ms/step - loss: 1.2431 - accuracy: 0.5974 - val_loss: 1.5047 - val_accuracy: 0.6037\n",
      "Epoch 50/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.2297 - accuracy: 0.6015\n",
      "Epoch 00050: val_loss did not improve from 1.47513\n",
      "251/251 [==============================] - 25s 98ms/step - loss: 1.2305 - accuracy: 0.6014 - val_loss: 1.4975 - val_accuracy: 0.5847\n",
      "Epoch 51/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.2124 - accuracy: 0.6064\n",
      "Epoch 00051: val_loss improved from 1.47513 to 1.47035, saving model to model.h5\n",
      "251/251 [==============================] - 27s 107ms/step - loss: 1.2127 - accuracy: 0.6061 - val_loss: 1.4703 - val_accuracy: 0.6142\n",
      "Epoch 52/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.2379 - accuracy: 0.5951\n",
      "Epoch 00052: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 28s 112ms/step - loss: 1.2378 - accuracy: 0.5953 - val_loss: 1.5117 - val_accuracy: 0.5982\n",
      "Epoch 53/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.1640 - accuracy: 0.6192\n",
      "Epoch 00053: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 28s 111ms/step - loss: 1.1639 - accuracy: 0.6193 - val_loss: 1.7232 - val_accuracy: 0.5767\n",
      "Epoch 54/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.2006 - accuracy: 0.6089\n",
      "Epoch 00054: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 24s 97ms/step - loss: 1.2006 - accuracy: 0.6089 - val_loss: 1.5368 - val_accuracy: 0.6012\n",
      "Epoch 55/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.1541 - accuracy: 0.6215\n",
      "Epoch 00055: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 23s 92ms/step - loss: 1.1544 - accuracy: 0.6214 - val_loss: 1.5209 - val_accuracy: 0.6107\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/251 [============================>.] - ETA: 0s - loss: 1.1245 - accuracy: 0.6335\n",
      "Epoch 00056: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 25s 98ms/step - loss: 1.1247 - accuracy: 0.6335 - val_loss: 1.5550 - val_accuracy: 0.6067\n",
      "Epoch 57/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.1541 - accuracy: 0.6230\n",
      "Epoch 00057: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 23s 93ms/step - loss: 1.1542 - accuracy: 0.6229 - val_loss: 1.5658 - val_accuracy: 0.6247\n",
      "Epoch 58/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.1561 - accuracy: 0.6220\n",
      "Epoch 00058: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 23s 91ms/step - loss: 1.1562 - accuracy: 0.6219 - val_loss: 1.5224 - val_accuracy: 0.6062\n",
      "Epoch 59/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.1351 - accuracy: 0.6343\n",
      "Epoch 00059: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 33s 132ms/step - loss: 1.1351 - accuracy: 0.6343 - val_loss: 1.5073 - val_accuracy: 0.6187\n",
      "Epoch 60/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.0873 - accuracy: 0.6424\n",
      "Epoch 00060: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 31s 124ms/step - loss: 1.0872 - accuracy: 0.6424 - val_loss: 1.4776 - val_accuracy: 0.6312\n",
      "Epoch 61/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.0194 - accuracy: 0.6604\n",
      "Epoch 00061: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 36s 142ms/step - loss: 1.0194 - accuracy: 0.6604 - val_loss: 1.5777 - val_accuracy: 0.6347\n",
      "Epoch 62/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 1.0158 - accuracy: 0.6665\n",
      "Epoch 00062: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 44s 177ms/step - loss: 1.0158 - accuracy: 0.6665 - val_loss: 1.6328 - val_accuracy: 0.6142\n",
      "Epoch 63/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.0513 - accuracy: 0.6538\n",
      "Epoch 00063: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 31s 125ms/step - loss: 1.0512 - accuracy: 0.6538 - val_loss: 1.5017 - val_accuracy: 0.6257\n",
      "Epoch 64/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9701 - accuracy: 0.6793\n",
      "Epoch 00064: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 27s 109ms/step - loss: 0.9699 - accuracy: 0.6794 - val_loss: 1.5477 - val_accuracy: 0.6417\n",
      "Epoch 65/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.1009 - accuracy: 0.6460\n",
      "Epoch 00065: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 26s 104ms/step - loss: 1.1010 - accuracy: 0.6459 - val_loss: 1.4792 - val_accuracy: 0.6252\n",
      "Epoch 66/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 1.0172 - accuracy: 0.6587\n",
      "Epoch 00066: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 30s 118ms/step - loss: 1.0170 - accuracy: 0.6588 - val_loss: 1.5560 - val_accuracy: 0.6372\n",
      "Epoch 67/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9788 - accuracy: 0.6741\n",
      "Epoch 00067: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 0.9789 - accuracy: 0.6741 - val_loss: 1.4987 - val_accuracy: 0.6517\n",
      "Epoch 68/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9742 - accuracy: 0.6798\n",
      "Epoch 00068: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 0.9742 - accuracy: 0.6797 - val_loss: 1.5598 - val_accuracy: 0.6397\n",
      "Epoch 69/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9502 - accuracy: 0.6834\n",
      "Epoch 00069: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 0.9499 - accuracy: 0.6835 - val_loss: 1.5739 - val_accuracy: 0.6452\n",
      "Epoch 70/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9876 - accuracy: 0.6722\n",
      "Epoch 00070: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 23s 90ms/step - loss: 0.9872 - accuracy: 0.6724 - val_loss: 1.5619 - val_accuracy: 0.6432\n",
      "Epoch 71/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9346 - accuracy: 0.6866\n",
      "Epoch 00071: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 0.9345 - accuracy: 0.6866 - val_loss: 1.6944 - val_accuracy: 0.6357\n",
      "Epoch 72/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9297 - accuracy: 0.6930\n",
      "Epoch 00072: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 0.9295 - accuracy: 0.6930 - val_loss: 1.5026 - val_accuracy: 0.6532\n",
      "Epoch 73/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.8860 - accuracy: 0.7044\n",
      "Epoch 00073: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 0.8859 - accuracy: 0.7045 - val_loss: 1.5553 - val_accuracy: 0.6477\n",
      "Epoch 74/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.8546 - accuracy: 0.7090\n",
      "Epoch 00074: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 0.8547 - accuracy: 0.7090 - val_loss: 1.7056 - val_accuracy: 0.6502\n",
      "Epoch 75/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.9324 - accuracy: 0.6861\n",
      "Epoch 00075: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 0.9325 - accuracy: 0.6860 - val_loss: 1.5975 - val_accuracy: 0.6427\n",
      "Epoch 76/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.8630 - accuracy: 0.7104\n",
      "Epoch 00076: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 27s 108ms/step - loss: 0.8630 - accuracy: 0.7104 - val_loss: 1.6073 - val_accuracy: 0.6567\n",
      "Epoch 77/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.8770 - accuracy: 0.7051\n",
      "Epoch 00077: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 27s 109ms/step - loss: 0.8770 - accuracy: 0.7051 - val_loss: 1.6431 - val_accuracy: 0.6532\n",
      "Epoch 78/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.8560 - accuracy: 0.7089\n",
      "Epoch 00078: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 28s 110ms/step - loss: 0.8560 - accuracy: 0.7089 - val_loss: 1.5997 - val_accuracy: 0.6537\n",
      "Epoch 79/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.8641 - accuracy: 0.7081\n",
      "Epoch 00079: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 28s 110ms/step - loss: 0.8641 - accuracy: 0.7081 - val_loss: 1.5131 - val_accuracy: 0.6602\n",
      "Epoch 80/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.8304 - accuracy: 0.7165\n",
      "Epoch 00080: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 28s 110ms/step - loss: 0.8304 - accuracy: 0.7165 - val_loss: 1.6747 - val_accuracy: 0.6562\n",
      "Epoch 81/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.8237 - accuracy: 0.7269\n",
      "Epoch 00081: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 28s 110ms/step - loss: 0.8237 - accuracy: 0.7269 - val_loss: 1.6330 - val_accuracy: 0.6602\n",
      "Epoch 82/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.8239 - accuracy: 0.7246\n",
      "Epoch 00082: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 28s 112ms/step - loss: 0.8239 - accuracy: 0.7246 - val_loss: 1.6481 - val_accuracy: 0.6627\n",
      "Epoch 83/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.8478 - accuracy: 0.7176\n",
      "Epoch 00083: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 28s 110ms/step - loss: 0.8478 - accuracy: 0.7176 - val_loss: 1.6536 - val_accuracy: 0.6622\n",
      "Epoch 84/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.7902 - accuracy: 0.7302\n",
      "Epoch 00084: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 28s 110ms/step - loss: 0.7902 - accuracy: 0.7302 - val_loss: 1.6115 - val_accuracy: 0.6717\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/251 [============================>.] - ETA: 0s - loss: 0.7700 - accuracy: 0.7398\n",
      "Epoch 00085: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 26s 102ms/step - loss: 0.7698 - accuracy: 0.7398 - val_loss: 1.7245 - val_accuracy: 0.6712\n",
      "Epoch 86/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.7938 - accuracy: 0.7293\n",
      "Epoch 00086: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 26s 102ms/step - loss: 0.7939 - accuracy: 0.7291 - val_loss: 1.6741 - val_accuracy: 0.6632\n",
      "Epoch 87/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.8106 - accuracy: 0.7311\n",
      "Epoch 00087: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 26s 103ms/step - loss: 0.8104 - accuracy: 0.7312 - val_loss: 1.6221 - val_accuracy: 0.6597\n",
      "Epoch 88/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.7570 - accuracy: 0.7386\n",
      "Epoch 00088: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 26s 102ms/step - loss: 0.7570 - accuracy: 0.7386 - val_loss: 1.6681 - val_accuracy: 0.6677\n",
      "Epoch 89/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.7953 - accuracy: 0.7360\n",
      "Epoch 00089: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 26s 102ms/step - loss: 0.7955 - accuracy: 0.7360 - val_loss: 1.6818 - val_accuracy: 0.6602\n",
      "Epoch 90/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.7630 - accuracy: 0.7441\n",
      "Epoch 00090: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 26s 102ms/step - loss: 0.7630 - accuracy: 0.7441 - val_loss: 1.6890 - val_accuracy: 0.6682\n",
      "Epoch 91/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.7548 - accuracy: 0.7373\n",
      "Epoch 00091: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 26s 105ms/step - loss: 0.7548 - accuracy: 0.7373 - val_loss: 1.6793 - val_accuracy: 0.6532\n",
      "Epoch 92/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.7248 - accuracy: 0.7535\n",
      "Epoch 00092: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 33s 131ms/step - loss: 0.7248 - accuracy: 0.7535 - val_loss: 1.7511 - val_accuracy: 0.6662\n",
      "Epoch 93/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.7258 - accuracy: 0.7514\n",
      "Epoch 00093: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 33s 130ms/step - loss: 0.7256 - accuracy: 0.7515 - val_loss: 1.7620 - val_accuracy: 0.6582\n",
      "Epoch 94/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.7029 - accuracy: 0.7599\n",
      "Epoch 00094: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 32s 128ms/step - loss: 0.7031 - accuracy: 0.7597 - val_loss: 1.7846 - val_accuracy: 0.6617\n",
      "Epoch 95/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.7322 - accuracy: 0.7490\n",
      "Epoch 00095: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 32s 129ms/step - loss: 0.7322 - accuracy: 0.7490 - val_loss: 1.7761 - val_accuracy: 0.6742\n",
      "Epoch 96/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.6708 - accuracy: 0.7701\n",
      "Epoch 00096: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 25s 101ms/step - loss: 0.6706 - accuracy: 0.7702 - val_loss: 1.7589 - val_accuracy: 0.6807\n",
      "Epoch 97/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.7676\n",
      "Epoch 00097: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 0.6624 - accuracy: 0.7676 - val_loss: 1.9200 - val_accuracy: 0.6647\n",
      "Epoch 98/100\n",
      "250/251 [============================>.] - ETA: 0s - loss: 0.8454 - accuracy: 0.7247\n",
      "Epoch 00098: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 22s 89ms/step - loss: 0.8453 - accuracy: 0.7249 - val_loss: 1.5800 - val_accuracy: 0.6552\n",
      "Epoch 99/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.7694 - accuracy: 0.7418\n",
      "Epoch 00099: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 33s 133ms/step - loss: 0.7694 - accuracy: 0.7418 - val_loss: 1.7003 - val_accuracy: 0.6607\n",
      "Epoch 100/100\n",
      "251/251 [==============================] - ETA: 0s - loss: 0.7005 - accuracy: 0.7565\n",
      "Epoch 00100: val_loss did not improve from 1.47035\n",
      "251/251 [==============================] - 37s 147ms/step - loss: 0.7005 - accuracy: 0.7565 - val_loss: 1.8107 - val_accuracy: 0.6732\n"
     ]
    }
   ],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "hist = model_lstm.fit(train_X, train_Y,\n",
    "                 epochs = 100,\n",
    "                 batch_size = 32,\n",
    "                 validation_data = (val_X, val_Y),\n",
    "                 callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YjXKos8ocXvw"
   },
   "outputs": [],
   "source": [
    "model_lstm = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSTEzrlzcuya"
   },
   "outputs": [],
   "source": [
    "def predictions(text):\n",
    "  clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n",
    "  test_word = word_tokenize(clean)\n",
    "  test_word = [w.lower() for w in test_word]\n",
    "  test_ls = word_tokenizer.texts_to_sequences(test_word)\n",
    "#   print(test_ls)\n",
    "#   print(test_word)\n",
    "\n",
    "  #Check for unknown words\n",
    "  if [] in test_ls:\n",
    "    test_ls = list(filter(None, test_ls))\n",
    "    \n",
    "  test_ls = np.array(test_ls).reshape(1, len(test_ls))\n",
    "  x = padding_doc(test_ls, max_length)\n",
    "    \n",
    "#   print(x)\n",
    "\n",
    "  pred = model_lstm.predict(x)\n",
    "  \n",
    "  return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1ddofshmdzK"
   },
   "outputs": [],
   "source": [
    "def get_final_output(pred, classes):\n",
    "  predictions = pred[0]\n",
    "#   print(predictions)\n",
    "  classes = np.array(classes)\n",
    "#   print(classes)\n",
    "  ids = np.argsort(-predictions)\n",
    "  classes = classes[ids]\n",
    "#   print(classes)\n",
    "  predictions = -np.sort(-predictions)\n",
    " \n",
    "  for i in range(pred.shape[1]):\n",
    "    print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))\n",
    "  \n",
    "  return classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "23VpGuihMdEU",
    "outputId": "cd36c932-0fb0-4166-92ae-546a7676e645"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card_arrival has confidence = 0.388203\n",
      "card_not_working has confidence = 0.2828172\n",
      "card_delivery_estimate has confidence = 0.10544735\n",
      "card_swallowed has confidence = 0.05497793\n",
      "declined_card_payment has confidence = 0.044158656\n",
      "activate_my_card has confidence = 0.03831381\n",
      "lost_or_stolen_card has confidence = 0.0148200765\n",
      "card_payment_fee_charged has confidence = 0.013619259\n",
      "compromised_card has confidence = 0.010870798\n",
      "card_acceptance has confidence = 0.010216601\n",
      "contactless_not_working has confidence = 0.009105929\n",
      "transaction_charged_twice has confidence = 0.007043392\n",
      "declined_cash_withdrawal has confidence = 0.004485206\n",
      "reverted_card_payment? has confidence = 0.004468125\n",
      "card_about_to_expire has confidence = 0.0038785327\n",
      "atm_support has confidence = 0.0020097475\n",
      "cash_withdrawal_not_recognised has confidence = 0.0011436713\n",
      "pin_blocked has confidence = 0.00084258296\n",
      "card_linking has confidence = 0.0007778044\n",
      "pending_card_payment has confidence = 0.00037651244\n",
      "lost_or_stolen_phone has confidence = 0.00028464865\n",
      "supported_cards_and_currencies has confidence = 0.00024930754\n",
      "transfer_not_received_by_recipient has confidence = 0.0002402179\n",
      "order_physical_card has confidence = 0.00022872793\n",
      "top_up_failed has confidence = 0.00021320276\n",
      "unable_to_verify_identity has confidence = 0.00015831574\n",
      "card_payment_not_recognised has confidence = 0.0001275453\n",
      "verify_top_up has confidence = 0.000112923575\n",
      "wrong_amount_of_cash_received has confidence = 9.3476425e-05\n",
      "beneficiary_not_allowed has confidence = 7.978111e-05\n",
      "virtual_card_not_working has confidence = 7.936987e-05\n",
      "visa_or_mastercard has confidence = 7.081243e-05\n",
      "topping_up_by_card has confidence = 6.9143105e-05\n",
      "why_verify_identity has confidence = 6.410678e-05\n",
      "receiving_money has confidence = 5.8417165e-05\n",
      "transfer_fee_charged has confidence = 5.161889e-05\n",
      "category has confidence = 4.8570604e-05\n",
      "top_up_by_card_charge has confidence = 3.677665e-05\n",
      "direct_debit_payment_not_recognised has confidence = 3.254498e-05\n",
      "cash_withdrawal_charge has confidence = 1.5222484e-05\n",
      "declined_transfer has confidence = 1.4111147e-05\n",
      "verify_source_of_funds has confidence = 1.3968804e-05\n",
      "balance_not_updated_after_cheque_or_cash_deposit has confidence = 1.2788762e-05\n",
      "country_support has confidence = 1.2633442e-05\n",
      "request_refund has confidence = 8.007759e-06\n",
      "passcode_forgotten has confidence = 7.0643696e-06\n",
      "pending_cash_withdrawal has confidence = 6.805021e-06\n",
      "Refund_not_showing_up has confidence = 5.7636207e-06\n",
      "pending_top_up has confidence = 5.47768e-06\n",
      "extra_charge_on_statement has confidence = 4.692897e-06\n",
      "apple_pay_or_google_pay has confidence = 3.3496876e-06\n",
      "top_up_by_cash_or_cheque has confidence = 3.142365e-06\n",
      "pending_transfer has confidence = 2.27504e-06\n",
      "getting_spare_card has confidence = 2.0953873e-06\n",
      "failed_transfer has confidence = 1.6219672e-06\n",
      "cancel_transfer has confidence = 1.5817341e-06\n",
      "age_limit has confidence = 1.2183998e-06\n",
      "get_physical_card has confidence = 6.209348e-07\n",
      "balance_not_updated_after_bank_transfer has confidence = 4.0194544e-07\n",
      "top_up_by_bank_transfer_charge has confidence = 3.3057705e-07\n",
      "top_up_limits has confidence = 2.8524903e-07\n",
      "exchange_charge has confidence = 2.2811807e-07\n",
      "change_pin has confidence = 1.1925746e-07\n",
      "terminate_account has confidence = 7.638907e-08\n",
      "card_payment_wrong_exchange_rate has confidence = 7.295355e-08\n",
      "top_up_reverted has confidence = 6.592595e-08\n",
      "disposable_card_limits has confidence = 6.458114e-08\n",
      "transfer_timing has confidence = 6.208445e-08\n",
      "getting_virtual_card has confidence = 5.8263574e-08\n",
      "wrong_exchange_rate_for_cash_withdrawal has confidence = 3.8958202e-08\n",
      "get_disposable_virtual_card has confidence = 2.9298132e-08\n",
      "automatic_top_up has confidence = 1.7530855e-08\n",
      "verify_my_identity has confidence = 4.8085464e-09\n",
      "edit_personal_details has confidence = 4.24786e-09\n",
      "transfer_into_account has confidence = 1.7479763e-09\n",
      "exchange_via_app has confidence = 6.7103306e-10\n",
      "exchange_rate has confidence = 2.7373395e-10\n",
      "fiat_currency_support has confidence = 9.198665e-11\n",
      "\n",
      "ans: card_arrival\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"I am still waiting on my card?\"\n",
    "pred = predictions(text)\n",
    "result = get_final_output(pred, unique_intent)\n",
    "print('\\nans: {}\\n'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exchange_rate has confidence = 0.99568826\n",
      "fiat_currency_support has confidence = 0.0020600427\n",
      "card_payment_wrong_exchange_rate has confidence = 0.0007059075\n",
      "exchange_charge has confidence = 0.00044949\n",
      "exchange_via_app has confidence = 0.00043904345\n",
      "supported_cards_and_currencies has confidence = 0.00037469165\n",
      "automatic_top_up has confidence = 0.00014319515\n",
      "wrong_exchange_rate_for_cash_withdrawal has confidence = 6.349546e-05\n",
      "verify_source_of_funds has confidence = 3.964243e-05\n",
      "top_up_by_bank_transfer_charge has confidence = 2.3390774e-05\n",
      "receiving_money has confidence = 9.816018e-06\n",
      "top_up_by_cash_or_cheque has confidence = 2.6036785e-06\n",
      "transfer_into_account has confidence = 3.3181928e-07\n",
      "wrong_amount_of_cash_received has confidence = 1.0008942e-07\n",
      "country_support has confidence = 3.2529005e-08\n",
      "atm_support has confidence = 2.884979e-08\n",
      "age_limit has confidence = 7.880087e-09\n",
      "visa_or_mastercard has confidence = 7.676242e-09\n",
      "edit_personal_details has confidence = 4.454245e-11\n",
      "why_verify_identity has confidence = 2.3783083e-11\n",
      "top_up_limits has confidence = 1.3222694e-11\n",
      "topping_up_by_card has confidence = 7.497638e-12\n",
      "top_up_by_card_charge has confidence = 6.2572387e-12\n",
      "declined_cash_withdrawal has confidence = 1.7460507e-12\n",
      "beneficiary_not_allowed has confidence = 2.0024822e-13\n",
      "transfer_timing has confidence = 1.6017215e-13\n",
      "balance_not_updated_after_bank_transfer has confidence = 1.0819944e-13\n",
      "unable_to_verify_identity has confidence = 2.9932643e-14\n",
      "verify_my_identity has confidence = 1.5170282e-14\n",
      "cash_withdrawal_charge has confidence = 3.370105e-15\n",
      "change_pin has confidence = 2.0260539e-15\n",
      "pending_top_up has confidence = 1.2086352e-16\n",
      "balance_not_updated_after_cheque_or_cash_deposit has confidence = 5.4488468e-17\n",
      "apple_pay_or_google_pay has confidence = 4.2240564e-17\n",
      "top_up_reverted has confidence = 2.5372475e-17\n",
      "transfer_not_received_by_recipient has confidence = 1.691216e-17\n",
      "transfer_fee_charged has confidence = 1.2677741e-17\n",
      "request_refund has confidence = 7.5638164e-18\n",
      "card_acceptance has confidence = 1.9110906e-18\n",
      "cancel_transfer has confidence = 4.7176664e-19\n",
      "cash_withdrawal_not_recognised has confidence = 1.5393866e-19\n",
      "pending_cash_withdrawal has confidence = 1.2155968e-19\n",
      "pin_blocked has confidence = 8.405174e-20\n",
      "terminate_account has confidence = 6.7384356e-20\n",
      "transaction_charged_twice has confidence = 3.435766e-20\n",
      "card_delivery_estimate has confidence = 3.2309048e-20\n",
      "card_payment_fee_charged has confidence = 4.05772e-21\n",
      "compromised_card has confidence = 1.6779491e-21\n",
      "top_up_failed has confidence = 1.6073338e-21\n",
      "passcode_forgotten has confidence = 9.006185e-22\n",
      "card_swallowed has confidence = 8.680244e-23\n",
      "category has confidence = 5.0724592e-23\n",
      "verify_top_up has confidence = 4.1468966e-23\n",
      "getting_spare_card has confidence = 1.8607152e-23\n",
      "declined_transfer has confidence = 1.7110354e-23\n",
      "disposable_card_limits has confidence = 1.0985871e-24\n",
      "card_about_to_expire has confidence = 1.1735461e-25\n",
      "card_not_working has confidence = 6.8078135e-26\n",
      "order_physical_card has confidence = 2.8571125e-26\n",
      "failed_transfer has confidence = 1.17727614e-26\n",
      "pending_transfer has confidence = 1.32100835e-27\n",
      "direct_debit_payment_not_recognised has confidence = 5.550096e-29\n",
      "lost_or_stolen_phone has confidence = 2.8060504e-29\n",
      "Refund_not_showing_up has confidence = 2.4943526e-30\n",
      "get_physical_card has confidence = 2.1240478e-30\n",
      "contactless_not_working has confidence = 1.10846785e-30\n",
      "activate_my_card has confidence = 7.0521964e-31\n",
      "get_disposable_virtual_card has confidence = 2.2207148e-31\n",
      "reverted_card_payment? has confidence = 5.9966134e-32\n",
      "card_arrival has confidence = 7.0723405e-33\n",
      "getting_virtual_card has confidence = 2.0352146e-33\n",
      "lost_or_stolen_card has confidence = 4.1976743e-35\n",
      "extra_charge_on_statement has confidence = 0.0\n",
      "card_linking has confidence = 0.0\n",
      "card_payment_not_recognised has confidence = 0.0\n",
      "pending_card_payment has confidence = 0.0\n",
      "declined_card_payment has confidence = 0.0\n",
      "virtual_card_not_working has confidence = 0.0\n",
      "\n",
      "ans: exchange_rate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"What are you exchange rates?\"\n",
    "pred = predictions(text)\n",
    "result = get_final_output(pred, unique_intent)\n",
    "print('\\nans: {}\\n'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country_support has confidence = 0.6209605\n",
      "getting_spare_card has confidence = 0.19558908\n",
      "visa_or_mastercard has confidence = 0.09848333\n",
      "order_physical_card has confidence = 0.03703459\n",
      "supported_cards_and_currencies has confidence = 0.014996924\n",
      "card_acceptance has confidence = 0.011216355\n",
      "atm_support has confidence = 0.007959759\n",
      "card_about_to_expire has confidence = 0.0064256815\n",
      "card_delivery_estimate has confidence = 0.0020956346\n",
      "verify_source_of_funds has confidence = 0.0013296802\n",
      "receiving_money has confidence = 0.0008137022\n",
      "disposable_card_limits has confidence = 0.0006901232\n",
      "lost_or_stolen_card has confidence = 0.00059694407\n",
      "top_up_by_card_charge has confidence = 0.00038836425\n",
      "card_payment_fee_charged has confidence = 0.00034286594\n",
      "card_linking has confidence = 0.00018339833\n",
      "age_limit has confidence = 0.00014364916\n",
      "card_not_working has confidence = 0.0001402933\n",
      "compromised_card has confidence = 0.00012902418\n",
      "get_disposable_virtual_card has confidence = 0.00012286741\n",
      "activate_my_card has confidence = 0.000101847836\n",
      "card_arrival has confidence = 3.3145945e-05\n",
      "passcode_forgotten has confidence = 3.2724434e-05\n",
      "exchange_charge has confidence = 2.3434855e-05\n",
      "pin_blocked has confidence = 2.332026e-05\n",
      "request_refund has confidence = 2.0753281e-05\n",
      "lost_or_stolen_phone has confidence = 1.7287444e-05\n",
      "transaction_charged_twice has confidence = 1.5983775e-05\n",
      "verify_my_identity has confidence = 1.5856484e-05\n",
      "why_verify_identity has confidence = 1.3770192e-05\n",
      "card_swallowed has confidence = 8.849964e-06\n",
      "top_up_by_bank_transfer_charge has confidence = 8.737579e-06\n",
      "getting_virtual_card has confidence = 8.45014e-06\n",
      "unable_to_verify_identity has confidence = 6.8755517e-06\n",
      "change_pin has confidence = 6.0821426e-06\n",
      "direct_debit_payment_not_recognised has confidence = 3.022773e-06\n",
      "exchange_via_app has confidence = 2.708857e-06\n",
      "apple_pay_or_google_pay has confidence = 2.4900198e-06\n",
      "contactless_not_working has confidence = 2.1297517e-06\n",
      "get_physical_card has confidence = 1.9792224e-06\n",
      "topping_up_by_card has confidence = 1.9756712e-06\n",
      "fiat_currency_support has confidence = 1.9016614e-06\n",
      "verify_top_up has confidence = 1.1685389e-06\n",
      "top_up_by_cash_or_cheque has confidence = 1.0191682e-06\n",
      "category has confidence = 7.088527e-07\n",
      "edit_personal_details has confidence = 6.0432455e-07\n",
      "top_up_limits has confidence = 2.5263523e-07\n",
      "cash_withdrawal_not_recognised has confidence = 6.2220664e-08\n",
      "top_up_failed has confidence = 4.6227978e-08\n",
      "automatic_top_up has confidence = 4.009399e-08\n",
      "transfer_fee_charged has confidence = 3.590364e-08\n",
      "exchange_rate has confidence = 1.6857475e-08\n",
      "wrong_amount_of_cash_received has confidence = 9.964909e-09\n",
      "declined_cash_withdrawal has confidence = 8.402458e-09\n",
      "virtual_card_not_working has confidence = 8.235117e-09\n",
      "beneficiary_not_allowed has confidence = 7.836106e-09\n",
      "reverted_card_payment? has confidence = 7.5193975e-09\n",
      "terminate_account has confidence = 1.888487e-09\n",
      "cash_withdrawal_charge has confidence = 1.4553015e-09\n",
      "declined_card_payment has confidence = 1.301755e-09\n",
      "card_payment_not_recognised has confidence = 1.2791823e-09\n",
      "cancel_transfer has confidence = 2.2392817e-10\n",
      "transfer_into_account has confidence = 9.25914e-11\n",
      "pending_top_up has confidence = 7.772256e-11\n",
      "card_payment_wrong_exchange_rate has confidence = 4.345842e-11\n",
      "Refund_not_showing_up has confidence = 3.9980297e-11\n",
      "transfer_not_received_by_recipient has confidence = 2.1575982e-11\n",
      "declined_transfer has confidence = 9.160099e-12\n",
      "wrong_exchange_rate_for_cash_withdrawal has confidence = 4.0394346e-12\n",
      "top_up_reverted has confidence = 3.835578e-12\n",
      "transfer_timing has confidence = 2.1661325e-12\n",
      "balance_not_updated_after_cheque_or_cash_deposit has confidence = 6.971039e-13\n",
      "extra_charge_on_statement has confidence = 4.01189e-13\n",
      "balance_not_updated_after_bank_transfer has confidence = 3.841675e-13\n",
      "pending_cash_withdrawal has confidence = 9.829834e-14\n",
      "failed_transfer has confidence = 6.4956094e-15\n",
      "pending_card_payment has confidence = 1.3637577e-15\n",
      "pending_transfer has confidence = 1.0066043e-15\n",
      "\n",
      "ans: country_support\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Which countries are represented?\"\n",
    "pred = predictions(text)\n",
    "result = get_final_output(pred, unique_intent)\n",
    "print('\\nans: {}\\n'.format(result))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Intent_classification_final.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitf6eaa932bd364e6c99622ad728a40cf7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
